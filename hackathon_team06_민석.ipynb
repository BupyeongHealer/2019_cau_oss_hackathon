{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hackathon_team06_민석",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BupyeongHealer/2019_cau_oss_hackathon/blob/master/hackathon_team06_%EB%AF%BC%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosAX9DXOlc",
        "colab_type": "text"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        " *  단, 사용할 데이터셋에 따라 is_mnist만 수정\n",
        "*   모든 구현은 [2. 데이터 전처리]와 [3. 모델 생성]에서만 진행\n",
        " *  데이터 전처리 후 트레이닝, 데이터 셋은 x_train_after, x_test_after 변수명을 유지해주세요.\n",
        " *  데이터셋이 달라져도 같은 모델 구조를 유지하여야함.\n",
        "*   [4. 모델 저장]과 [5. 모델 로드 및 평가]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *  트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        " *  team_name을 제외한 다른 부분은 수정하지 말 것\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임->모든 런타임 재설정\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  모델 구조 파일 (model_structure_teamXX.json)\n",
        " *  모델 weight 파일 (model_weight_teamXX.h5)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        "* 제출 기한: **오후 6시**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2019_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드 or 알고리즘이 적발될 경우\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  두 개의 데이터셋에 대해 다른 모델 구조를 사용한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lwEXhUqys1",
        "colab_type": "text"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5PBBJ1qSC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "737aafe0-cb1a-4e89-9b48-0e388d0a4794"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "is_mnist = True;\n",
        "\n",
        "# 데이터셋 로드\n",
        "# x_train, y_train: 트레이닝 데이터 및 레이블\n",
        "# x_test, y_test: 테스트 데이터 및 레이블\n",
        "if is_mnist:\n",
        "  data_type = 'mnist'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() # fashion MNIST 데이터셋인 경우,\n",
        "else:\n",
        "  data_type = 'cifar10'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # cifar10 데이터셋인 경우,\n",
        "\n",
        "\n",
        "# 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 총 클래스 개수\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# 인풋 데이터 타입\n",
        "input_shape = x_test.shape[1:]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9c2KLDBIhNQ",
        "colab_type": "text"
      },
      "source": [
        "# **2. 데이터 전처리**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJNgjaHvIhSS",
        "colab_type": "code",
        "outputId": "22c4cfe2-5275-4f42-b009-2f5d33a31891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "x_train_after = x_train / 255.0\n",
        "x_test_after = x_test / 255.0\n",
        "if is_mnist == True:\n",
        "  input_shape = (x_train.shape[1],x_train.shape[2],1)\n",
        "  x_train_after = np.reshape(x_train_after,(60000,28,28,1))\n",
        "  x_test_after = np.reshape(x_test_after,(10000,28,28,1))\n",
        "  \n",
        "  \n",
        "####  \n",
        "# Standardize images across the dataset, mean=0, stdev=1\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('th')\n",
        "\n",
        "# convert from int to float\n",
        "x_train_after = x_train_after.astype('float32')\n",
        "x_test_after = x_test_after.astype('float32')\n",
        "# define data preparation\n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
        "#datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "#datagen = ImageDataGenerator(zca_whitening=True)\n",
        "\n",
        "# fit parameters from data\n",
        "datagen.fit(x_train_after)\n",
        "# configure batch size and retrieve one batch of images\n",
        "\n",
        "#####  \n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:940: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3 or 4 channels on axis 1. However, it was passed an array with shape (60000, 28, 28, 1) (28 channels).\n",
            "  ' channels).')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YjppJpXBO9",
        "colab_type": "text"
      },
      "source": [
        "# **3. 모델 생성**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xf1cGW6gui9",
        "colab_type": "code",
        "outputId": "0056cf5e-9809-412e-ee5f-4d010025e0cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import optimizers\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Conv2D(32, kernel_initializer='he_normal', kernel_size=(3,3), strides=(1,1), padding='same', input_shape = input_shape))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Conv2D(32, kernel_initializer='he_normal', kernel_size=(3,3), strides=(1,1), padding='same', input_shape = input_shape))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "model.add(keras.layers.Conv2D(64, kernel_initializer='he_normal', kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(keras.layers.Conv2D(64, kernel_initializer='he_normal', kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(keras.layers.Conv2D(128, kernel_initializer='he_normal', kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.4))\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(500, kernel_initializer='he_normal'))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(keras.layers.Dense(num_classes, kernel_initializer='he_normal', activation='softmax'))\n",
        "\n",
        "# 모델 컴파일\n",
        "# optimizer: 모델을 업데이트 하는 방식\n",
        "# loss: 모델의 정확도를 판단하는 방식\n",
        "# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "adam = optimizers.Adam(lr=0.005,beta_1=0.9,beta_2=0.999,epsilon=None, decay=1e-6, amsgrad=False)\n",
        "\n",
        "save_path = '/content/'\n",
        "team_name = 'team06'\n",
        "cd_checkpoint = keras.callbacks.ModelCheckpoint(filepath=save_path +  'model_entire_' + data_type + '_' + team_name + '.h5', monitor='val_acc', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', learning_rate=0.001, optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n",
        "# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set\n",
        "model.fit(x_train_after, y_train, batch_size = 100, epochs = 50, shuffle=True, validation_data=[x_test_after, y_test], callbacks=[cd_checkpoint])\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + data_type + '_' + team_name + '.h5')\n",
        "print('model recall')\n",
        "model.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\n",
        "print('weight save')\n",
        "model_json = model.to_json()\n",
        "with open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \n",
        "    json_file.write(model_json)\n",
        "print('json save')\n",
        "\n",
        "save_path = '/content/'\n",
        "team_name = 'team06'\n",
        "\n",
        "# 모델의 weight 값만 저장합니다.\n",
        "model.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\n",
        "\n",
        "# 모델의 구조만을 저장합니다.\n",
        "model_json = model.to_json()\n",
        "with open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \n",
        "    json_file.write(model_json)\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_' + data_type + '_' + team_name + '.h5')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "59600/60000 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.7116\n",
            "Epoch 00001: val_acc improved from -inf to 0.82610, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 8s 137us/sample - loss: 0.0401 - acc: 0.7123 - val_loss: 0.0250 - val_acc: 0.8261\n",
            "Epoch 2/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.8304\n",
            "Epoch 00002: val_acc improved from 0.82610 to 0.86720, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0247 - acc: 0.8304 - val_loss: 0.0195 - val_acc: 0.8672\n",
            "Epoch 3/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.8568\n",
            "Epoch 00003: val_acc improved from 0.86720 to 0.87790, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0207 - acc: 0.8570 - val_loss: 0.0179 - val_acc: 0.8779\n",
            "Epoch 4/50\n",
            "59800/60000 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.8711\n",
            "Epoch 00004: val_acc improved from 0.87790 to 0.89130, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 121us/sample - loss: 0.0188 - acc: 0.8713 - val_loss: 0.0160 - val_acc: 0.8913\n",
            "Epoch 5/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.8801\n",
            "Epoch 00005: val_acc improved from 0.89130 to 0.90080, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 121us/sample - loss: 0.0174 - acc: 0.8801 - val_loss: 0.0145 - val_acc: 0.9008\n",
            "Epoch 6/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.8892\n",
            "Epoch 00006: val_acc did not improve from 0.90080\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0161 - acc: 0.8892 - val_loss: 0.0151 - val_acc: 0.8983\n",
            "Epoch 7/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.8945\n",
            "Epoch 00007: val_acc did not improve from 0.90080\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0154 - acc: 0.8946 - val_loss: 0.0151 - val_acc: 0.8972\n",
            "Epoch 8/50\n",
            "59700/60000 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.8984\n",
            "Epoch 00008: val_acc improved from 0.90080 to 0.91150, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0148 - acc: 0.8984 - val_loss: 0.0131 - val_acc: 0.9115\n",
            "Epoch 9/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9047\n",
            "Epoch 00009: val_acc did not improve from 0.91150\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0141 - acc: 0.9046 - val_loss: 0.0143 - val_acc: 0.9018\n",
            "Epoch 10/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9056\n",
            "Epoch 00010: val_acc improved from 0.91150 to 0.91330, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0137 - acc: 0.9056 - val_loss: 0.0128 - val_acc: 0.9133\n",
            "Epoch 11/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9090\n",
            "Epoch 00011: val_acc improved from 0.91330 to 0.91670, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0134 - acc: 0.9090 - val_loss: 0.0122 - val_acc: 0.9167\n",
            "Epoch 12/50\n",
            "59700/60000 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9120\n",
            "Epoch 00012: val_acc did not improve from 0.91670\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0130 - acc: 0.9119 - val_loss: 0.0124 - val_acc: 0.9163\n",
            "Epoch 13/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9138\n",
            "Epoch 00013: val_acc improved from 0.91670 to 0.91950, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0126 - acc: 0.9137 - val_loss: 0.0121 - val_acc: 0.9195\n",
            "Epoch 14/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9168\n",
            "Epoch 00014: val_acc did not improve from 0.91950\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0123 - acc: 0.9168 - val_loss: 0.0119 - val_acc: 0.9181\n",
            "Epoch 15/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9187\n",
            "Epoch 00015: val_acc improved from 0.91950 to 0.92100, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0121 - acc: 0.9187 - val_loss: 0.0118 - val_acc: 0.9210\n",
            "Epoch 16/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9203\n",
            "Epoch 00016: val_acc did not improve from 0.92100\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0118 - acc: 0.9204 - val_loss: 0.0119 - val_acc: 0.9192\n",
            "Epoch 17/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9214\n",
            "Epoch 00017: val_acc did not improve from 0.92100\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0116 - acc: 0.9214 - val_loss: 0.0124 - val_acc: 0.9147\n",
            "Epoch 18/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9212\n",
            "Epoch 00018: val_acc improved from 0.92100 to 0.92530, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0116 - acc: 0.9211 - val_loss: 0.0110 - val_acc: 0.9253\n",
            "Epoch 19/50\n",
            "59600/60000 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9246\n",
            "Epoch 00019: val_acc improved from 0.92530 to 0.92650, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0111 - acc: 0.9244 - val_loss: 0.0112 - val_acc: 0.9265\n",
            "Epoch 20/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9251\n",
            "Epoch 00020: val_acc improved from 0.92650 to 0.92780, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0110 - acc: 0.9252 - val_loss: 0.0108 - val_acc: 0.9278\n",
            "Epoch 21/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9269\n",
            "Epoch 00021: val_acc improved from 0.92780 to 0.92860, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0109 - acc: 0.9269 - val_loss: 0.0106 - val_acc: 0.9286\n",
            "Epoch 22/50\n",
            "59600/60000 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9269\n",
            "Epoch 00022: val_acc did not improve from 0.92860\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0108 - acc: 0.9271 - val_loss: 0.0117 - val_acc: 0.9208\n",
            "Epoch 23/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9288\n",
            "Epoch 00023: val_acc did not improve from 0.92860\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0107 - acc: 0.9288 - val_loss: 0.0108 - val_acc: 0.9268\n",
            "Epoch 24/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9293\n",
            "Epoch 00024: val_acc did not improve from 0.92860\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0105 - acc: 0.9293 - val_loss: 0.0105 - val_acc: 0.9283\n",
            "Epoch 25/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9316\n",
            "Epoch 00025: val_acc did not improve from 0.92860\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0102 - acc: 0.9317 - val_loss: 0.0111 - val_acc: 0.9269\n",
            "Epoch 26/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9312\n",
            "Epoch 00026: val_acc improved from 0.92860 to 0.92930, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0102 - acc: 0.9312 - val_loss: 0.0106 - val_acc: 0.9293\n",
            "Epoch 27/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9313\n",
            "Epoch 00027: val_acc did not improve from 0.92930\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0102 - acc: 0.9313 - val_loss: 0.0110 - val_acc: 0.9260\n",
            "Epoch 28/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9339\n",
            "Epoch 00028: val_acc improved from 0.92930 to 0.93040, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0099 - acc: 0.9339 - val_loss: 0.0105 - val_acc: 0.9304\n",
            "Epoch 29/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9336\n",
            "Epoch 00029: val_acc did not improve from 0.93040\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0098 - acc: 0.9336 - val_loss: 0.0110 - val_acc: 0.9268\n",
            "Epoch 30/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9354\n",
            "Epoch 00030: val_acc did not improve from 0.93040\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0097 - acc: 0.9354 - val_loss: 0.0107 - val_acc: 0.9285\n",
            "Epoch 31/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9353\n",
            "Epoch 00031: val_acc did not improve from 0.93040\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0096 - acc: 0.9354 - val_loss: 0.0106 - val_acc: 0.9284\n",
            "Epoch 32/50\n",
            "59600/60000 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9365\n",
            "Epoch 00032: val_acc did not improve from 0.93040\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0095 - acc: 0.9365 - val_loss: 0.0106 - val_acc: 0.9285\n",
            "Epoch 33/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9376\n",
            "Epoch 00033: val_acc improved from 0.93040 to 0.93250, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0094 - acc: 0.9377 - val_loss: 0.0102 - val_acc: 0.9325\n",
            "Epoch 34/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9375\n",
            "Epoch 00034: val_acc did not improve from 0.93250\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0095 - acc: 0.9375 - val_loss: 0.0112 - val_acc: 0.9252\n",
            "Epoch 35/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9368\n",
            "Epoch 00035: val_acc did not improve from 0.93250\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0094 - acc: 0.9367 - val_loss: 0.0101 - val_acc: 0.9310\n",
            "Epoch 36/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9387\n",
            "Epoch 00036: val_acc did not improve from 0.93250\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0092 - acc: 0.9387 - val_loss: 0.0101 - val_acc: 0.9320\n",
            "Epoch 37/50\n",
            "59600/60000 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9397\n",
            "Epoch 00037: val_acc did not improve from 0.93250\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0091 - acc: 0.9396 - val_loss: 0.0108 - val_acc: 0.9289\n",
            "Epoch 38/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9386\n",
            "Epoch 00038: val_acc improved from 0.93250 to 0.93300, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0092 - acc: 0.9386 - val_loss: 0.0099 - val_acc: 0.9330\n",
            "Epoch 39/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9401\n",
            "Epoch 00039: val_acc did not improve from 0.93300\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0091 - acc: 0.9401 - val_loss: 0.0106 - val_acc: 0.9295\n",
            "Epoch 40/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9397\n",
            "Epoch 00040: val_acc did not improve from 0.93300\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0091 - acc: 0.9396 - val_loss: 0.0112 - val_acc: 0.9261\n",
            "Epoch 41/50\n",
            "59600/60000 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9409\n",
            "Epoch 00041: val_acc did not improve from 0.93300\n",
            "60000/60000 [==============================] - 7s 122us/sample - loss: 0.0089 - acc: 0.9409 - val_loss: 0.0100 - val_acc: 0.9324\n",
            "Epoch 42/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9410\n",
            "Epoch 00042: val_acc did not improve from 0.93300\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0089 - acc: 0.9412 - val_loss: 0.0106 - val_acc: 0.9322\n",
            "Epoch 43/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9403\n",
            "Epoch 00043: val_acc did not improve from 0.93300\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0089 - acc: 0.9402 - val_loss: 0.0101 - val_acc: 0.9318\n",
            "Epoch 44/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9421\n",
            "Epoch 00044: val_acc improved from 0.93300 to 0.93390, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0087 - acc: 0.9422 - val_loss: 0.0100 - val_acc: 0.9339\n",
            "Epoch 45/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9425\n",
            "Epoch 00045: val_acc did not improve from 0.93390\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0086 - acc: 0.9424 - val_loss: 0.0103 - val_acc: 0.9315\n",
            "Epoch 46/50\n",
            "59800/60000 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9428\n",
            "Epoch 00046: val_acc did not improve from 0.93390\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0086 - acc: 0.9429 - val_loss: 0.0101 - val_acc: 0.9329\n",
            "Epoch 47/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9427\n",
            "Epoch 00047: val_acc improved from 0.93390 to 0.93500, saving model to /content/model_entire_mnist_team06.h5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0087 - acc: 0.9427 - val_loss: 0.0098 - val_acc: 0.9350\n",
            "Epoch 48/50\n",
            "59900/60000 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9436\n",
            "Epoch 00048: val_acc did not improve from 0.93500\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0085 - acc: 0.9436 - val_loss: 0.0101 - val_acc: 0.9347\n",
            "Epoch 49/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9440\n",
            "Epoch 00049: val_acc did not improve from 0.93500\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0084 - acc: 0.9440 - val_loss: 0.0103 - val_acc: 0.9311\n",
            "Epoch 50/50\n",
            "59500/60000 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9453\n",
            "Epoch 00050: val_acc did not improve from 0.93500\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0083 - acc: 0.9453 - val_loss: 0.0100 - val_acc: 0.9344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0824 08:58:38.678387 140435893176192 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0824 08:58:38.679906 140435893176192 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0824 08:58:38.704325 140435893176192 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model recall\n",
            "weight save\n",
            "json save\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR9WUYXxqtfR",
        "colab_type": "text"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9yznz4qvzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team06'\n",
        "\n",
        "# 모델의 weight 값만 저장합니다.\n",
        "model.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\n",
        "\n",
        "# 모델의 구조만을 저장합니다.\n",
        "model_json = model.to_json()\n",
        "with open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \n",
        "    json_file.write(model_json)\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_' + data_type + '_' + team_name + '.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2BoRDZ7cFl",
        "colab_type": "text"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDBwxVUx7knQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e4a6513-8475-48dd-fb33-3e9cdceefadf"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team06'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + data_type + '_' + team_name + '.h5')\n",
        "model.summary()\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 3, 3, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 500)               64500     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 209,638\n",
            "Trainable params: 208,998\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n",
            "10000/10000 [==============================] - 1s 97us/sample - loss: 0.0098 - acc: 0.9350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.009845526255376172, 0.935]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}